{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- TODO set up dummy network\n",
    "-- TODO set up mask-making code\n",
    "-- TODO set up criterion code \n",
    "-- TODO test noopmask+criterion against regular critierion\n",
    "-- TODO test real mask with dummy annealing\n",
    "-- TODO test real thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = torch.rand(1,32,32) -- pass a random tensor as input to the network\n",
    "output = net:forward(input)\n",
    "--print(input)\n",
    "print(output:exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion() -- a negative log-likelihood criterion for multi-class classification\n",
    "error = criterion:forward(output, 3) -- let's say the groundtruth was class number: 3\n",
    "gradients = criterion:backward(output, 4)\n",
    "gradInput = net:backward(input, gradients)\n",
    "print(error)\n",
    "print(criterion)\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asdf = torch.ones(10)\n",
    "blah = torch.zeros(10)\n",
    "blah[7] = 1\n",
    "qwer = torch.cmul(blah, asdf)\n",
    "print(blah)\n",
    "print(asdf)\n",
    "print(qwer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- TODO I don't think using cmul is correct. You have to make sure you multiply each result forward by the mask.\n",
    "-- TODO test this out on some REAL data from provider.\n",
    "\n",
    "-- TODO there are special functions for masks, e.g. maskedCopy, maskedFill, try those...\n",
    "\n",
    "require 'nn';\n",
    "\n",
    "do\n",
    "\n",
    "-- Construct a new one of these every mini batch, because you need a new unlabledMask for each one.\n",
    "-- This assumes unlabeled have labeled of 0.\n",
    "local PseudoLabelCriterion, Criterion = torch.class('nn.PseudoLabelCriterion', 'nn.Criterion')\n",
    "\n",
    "-- crit: Criterion object\n",
    "-- unlabeledMask: a 1 x n tensor, where n is the mini-batch size.\n",
    "--                It has zeros for labeled data, and ones for unlabeled data.\n",
    "-- kAnneal: a scalar we use to weight the unlabled components of the gradient of the loss \n",
    "function PseudoLabelCriterion:__init(crit, unlabeledMask, kAnneal)\n",
    "   Criterion.__init(self)\n",
    "   self.crit = crit\n",
    "   self.unlabeledMask = unlabeledMask -- TODO maybe this should be a long()\n",
    "   -- The unlabled mask, but the ones are replaced with kAnneal, and zeros are replaced with ones.\n",
    "        -- TODO do I have to cuda this?\n",
    "   if (type(unlabeledMask) == 'number') then\n",
    "      self.annealedUnlabeledMask = self.unlabeledMask*(kAnneal-1) + 1\n",
    "   else\n",
    "      -- putting this into a diagonal matrix allows you to multiply rows easily\n",
    "      self.annealedUnlabeledMask = self.unlabeledMask*(kAnneal-1) + torch.ones(self.unlabeledMask:size())\n",
    "   end\n",
    "end\n",
    "\n",
    "\n",
    "-- returns/sets the error\n",
    "-- NOTE: modifies target. we assume target has zeros for the unlabeled data.\n",
    "-- input must be n x k matrix, where n is the mini batch size and k is the number of classes.\n",
    "-- target must be a 1 x n matrix with the correct labels\n",
    "function PseudoLabelCriterion:updateOutput(input, target)\n",
    "   -- squeeze gets rid of extra dimensions.\n",
    "   input = input:squeeze()\n",
    "   if type(target) == 'number' then -- single dimension case\n",
    "      if unlabeledMask == 1 then\n",
    "         _, predictedLabel = torch.max(input, 1)\n",
    "         target = predictedLabel\n",
    "      end\n",
    "      self.crit:updateOutput(input, target)\n",
    "      self.crit.output = self.crit.output * self.annealedUnlabeledMask\n",
    "   else\n",
    "      target = target:squeeze()\n",
    "      -- Get the max indexes of the target, which gets you the predicted labels      \n",
    "      _, predictedLabels = torch.max(input, 2)\n",
    "      predictedLabels = predictedLabels:float() -- TODO do I have to cuda this?\n",
    "      -- keep the predicted labels of the unlabeled data.\n",
    "      -- This assumes the unlabeled data has a target of zero.   \n",
    "      target:add(torch.cmul(predictedLabels, self.unlabeledMask))\n",
    "      self.crit:updateOutput(input, target)\n",
    "      -- multiplies unlabeled entries by kAnneal.      \n",
    "      self.crit.output:cmul(self.annealedUnlabeledMask)\n",
    "   end\n",
    "   self.output = self.crit.output\n",
    "   return self.output\n",
    "end\n",
    "\n",
    "-- TODO no idea if this is correct.\n",
    "-- returns/sets the gradient\n",
    "function PseudoLabelCriterion:updateGradInput(input, target)\n",
    "   input = input:squeeze()\n",
    "   if type(target) == 'number' then \n",
    "      self.crit:updateGradInput(input, target)\n",
    "      self.crit.gradInput = self.crit.gradInput * self.annealedUnlabeledMask      \n",
    "   else\n",
    "      target = target:squeeze()\n",
    "      self.crit:updateGradInput(input, target)\n",
    "      -- TODO SHOULD THIS BE MUL???        \n",
    "            -- TODO do you nead to take the diag + transpose here?\n",
    "      self.crit.gradInput:cmul(self.annealedUnlabeledMask) -- multiplies unlabeled entries by kAnneal.\n",
    "   end     \n",
    "   self.gradInput = self.crit.gradInput\n",
    "   return self.gradInput\n",
    "end\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==> loading data\t\n",
       "provider.tiny.evaluate.t7\t\n",
       "loading data from file...\t\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'provider'\n",
    "\n",
    "p = load_provider('tiny', 'evaluate', false)\n",
    "data = p.trainData.data\n",
    "labels = p.trainData.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model:add(dofile('models/sample.lua'))\n",
    "model:training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0070  0.0114 -0.5080 -0.2450 -0.0026  0.4792  0.0237  0.3736 -0.4440  0.3248\n",
       "-0.0490 -0.0236  0.1370 -0.0756 -0.6475  0.2812  0.6069 -0.2230  0.1334  0.1203\n",
       " 0.0495  0.2698 -0.0844  0.1220 -0.3436  0.0022  0.0825 -0.6721  0.0897 -0.2334\n",
       "[torch.FloatTensor of size 3x10]\n",
       "\n",
       " 1\n",
       " 0\n",
       " 1\n",
       "[torch.FloatTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 3\n",
    "batch = torch.range(1, batchSize):long()\n",
    "inputs = data:index(1, batch)\n",
    "targets = torch.FloatTensor(batchSize)\n",
    "-- targets are all ones to start\n",
    "targets:copy(labels:index(1, batch))\n",
    "targets[2] = 0 -- make the 2nd one unlabeled\n",
    "\n",
    "predictions = model:forward(inputs)\n",
    "\n",
    "print(predictions)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- TODO \n",
    "\n",
    "unlabeledMask = torch.zeros(3)\n",
    "unlabeledMask[2] = 1 -- 2nd one unlabeled\n",
    "print('unlabeled mask')\n",
    "print(unlabeledMask)\n",
    "kAnneal = 2\n",
    "\n",
    "_, predictedLabels = torch.max(predictions, 2)\n",
    "predictedLabels = predictedLabels:float()\n",
    "print('model predictions')\n",
    "print(predictedLabels)\n",
    "\n",
    "print('original targets')\n",
    "print(targets)\n",
    "\n",
    "pseudoTargets = torch.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unlabeled mask\t\n",
       " 0\n",
       " 1\n",
       " 0\n",
       "[torch.FloatTensor of size 3]\n",
       "\n",
       "model predictions\t\n",
       " 6\n",
       " 7\n",
       " 2\n",
       "[torch.FloatTensor of size 3x1]\n",
       "\n",
       "original targets\t\n",
       " 1\n",
       " 0\n",
       " 1\n",
       "[torch.FloatTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "[string \"-- TODO ...\"]:17: attempt to call field 'CudaTensor' (a nil value)\nstack traceback:\n\t[string \"-- TODO ...\"]:17: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/pinesol/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/pinesol/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/pinesol/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/pinesol/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/pinesol/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/pinesol/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/pinesol/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0108ab1d50",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- TODO ...\"]:17: attempt to call field 'CudaTensor' (a nil value)\nstack traceback:\n\t[string \"-- TODO ...\"]:17: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/pinesol/torch/install/share/lua/5.1/itorch/main.lua:179: in function </Users/pinesol/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/pinesol/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/pinesol/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/pinesol/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/pinesol/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/pinesol/torch/install/share/lua/5.1/itorch/main.lua:350: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0108ab1d50"
     ]
    }
   ],
   "source": [
    "-- TODO \n",
    "\n",
    "unlabeledMask = torch.zeros(3)\n",
    "unlabeledMask[2] = 1 -- 2nd one unlabeled\n",
    "print('unlabeled mask')\n",
    "print(unlabeledMask)\n",
    "kAnneal = 2\n",
    "\n",
    "_, predictedLabels = torch.max(predictions, 2)\n",
    "predictedLabels = predictedLabels:float()\n",
    "print('model predictions')\n",
    "print(predictedLabels)\n",
    "\n",
    "print('original targets')\n",
    "print(targets)\n",
    "\n",
    "pseudoTargets = torch.FloatTensor(targets:size())\n",
    "pseudoTargets:copy(targets)\n",
    "pseudoTargets:add(torch.cmul(predictedLabels, unlabeledMask))\n",
    "\n",
    "print('pseudo targets')\n",
    "print(pseudoTargets)\n",
    "\n",
    "criterion = nn.CrossEntropyCriterion()\n",
    "error = criterion:forward(predictedLabels, pseudoTargets)\n",
    "print('error')\n",
    "print(error)\n",
    "\n",
    "print('annealed error')\n",
    "annealedUnlabeledMask = unlabeledMask*(kAnneal-1) + torch.ones(unlabeledMask:size())\n",
    "error:cmul(annealedUnlabeledMask)\n",
    "print(error)\n",
    "\n",
    "-- TODO OH SHIT. the error here isn't a vector with errors for each data point.\n",
    "-- Error is computed over all the data points in the batch\n",
    "-- fuuuuuuuuuuuuck this kills the whole criterion idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- TODO test jacobian\n",
    "\n",
    "-- parameters\n",
    "local precision = 1e-5\n",
    "local jac = nn.Jacobian\n",
    "\n",
    "-- define inputs and module\n",
    "local ini = math.random(10,20)\n",
    "local inj = math.random(10,20)\n",
    "local ink = math.random(10,20)\n",
    "local percentage = 0.5\n",
    "local input = torch.Tensor(ini,inj,ink):zero()\n",
    "local module = nn.Dropout(percentage)\n",
    "\n",
    "-- test backprop, with Jacobian\n",
    "local err = jac.testJacobian(module,input)\n",
    "print('==> error: ' .. err)\n",
    "if err<precision then\n",
    "   print('==> module OK')\n",
    "else\n",
    "      print('==> error too large, incorrect implementation')\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
