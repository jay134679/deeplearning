

* Do initial class paperwork
** TODO Decide team name
** TODO Do we need a leader?
** TODO Formally create team by emailing TA
If you are still forming a team, please introduce your team by sending an email to Jake
j.zhao@nyu.edu with the following subject line and specify the team name, team members,
and their NetIDs.
[DS-GA-1008 YOUR_TEAM_NAME]

** TODO Who is our "corresponding TA"?


* DONE prepare programming environment
** DONE install lua and torch
** DONE create git repo
** DONE move this file to the git repo
** DONE install itorch notebook
https://github.com/facebook/iTorch#installing-itorch
** DONE set up cims environment
I already have an account, figure out how to log in.
if you can't use: 
https://www.cims.nyu.edu/webapps/content/systems/userservices/accounts/obtain
** DONE Set up CIMS web server for model submission
http://cims.nyu.edu/webapps/content/systems/userservices/webhosting


* DONE Week 1 Studying
** DONE Review slides from Lecture, make questions
** TODO Watch video / read slides from ICML tutorial

** TODO read lab notebook on tensors in torch
https://github.com/mayanks43/NYU-DL-2016/blob/master/Tensors.ipynb


* DONE Week 2 Studying
** TODO Read book about backprop
http://neuralnetworksanddeeplearning.com/chap2.html
** TODO read the convnet paper
http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf 
** TODO Go over lecture 2 slides
   

* DONE homework 1
http://cilvr.nyu.edu/lib/exe/fetch.php?media=deeplearning:2016:a1.pdf
** DONE Find solutions for problem 1
*** DONE Solve problem 1.1
*** DONE Solve problem 1.2
*** CANCELLED Solve problem 1.3
** DONE Write up solutions for problem 1
Use itorch notebook
** DONE Go through itorch notebook tutorial
http://code.madbits.com/wiki/doku.php?id=tutorial_supervised
*** DONE 1. Data prep
**** TODO What is the difference between the colon and dot operators?
**** TODO what is the hastag/pound operator?
**** TODO I don't understand the following syntax:
trainData.data[{ {},i,{},{} }]
trainData.data[{ i,{1},{},{} }]
**** TODO image.gaussian1D(7) ?
**** TODO I don't understand this normalization technique
*** DONE 2. Model prep
**** TODO What does "nn.Reshape(ninputs)" mean?
**** TODO nn.Linear(ninputs, noutputs)?
**** TODO nn.tables.random(nfeats, nstates[1], fanin[1])?
**** TODO Understand this conceptually
*** 3. Loss function
Nothing todo here
*** DONE 4. Training
**** TODO Don't understand the usage of the 'or' keyword
epoch = epoch or 1
**** TODO What does the 'local' keyword do?
local time = sys.clock()
**** TODO What does the ~= operator do?
if x ~= parameters then
**** TODO Understand this conceptually
*** 5. Testing
nothing new to do
** DONE Read and understand MNIST sample code
https://github.com/yjxiao/ds-ga-1008-a1
** DONE Run MNIST code on CIMSa
running locally instead

you@crunchy1[Documents]$ git clone https://github.com/yjxiao/ds-ga-1008-a1
you@crunchy1[Documents]$ module load torch
you@crunchy1[Documents]$ cd ds-ga-1008-a1
you@crunchy1[ds-ga-1008-a1]$ th doall.lua

works, but slowly, trying on cims to see if it's faster

actually, it's slower on cims!


** DONE Why don't the results from result.lua match those from doall?
** DONE improve performance of given model
*** DONE Add validation as suggested in the testing part of the tutorial
*** DONE Get the model to stop training once it 'converges'

*** DONE create script result.lua that generates predictions.csv
*** DONE convert and save the pictures
*** DONE add graphs to notebook
*** DONE save your changes to git
*** DONE Try the different model, loss function, and training ideas from the tutorial
** DONE do a full run of the simple model on the validation set so we have something to turn in
** DONE compare predictions.csv to someone else
** DONE write report on your model structure, training process, experiments, results, etc.
We expect a rather formal report written with Latex
use itorch notebook
** DONE Expose trained model file via CIMS
http://cims.nyu.edu/webapps/content/systems/userservices/webhosting f
** DONE Submit predictions.csv to Kaggle
** DONE Email final submission to TA
predictions.csv + result.lua + writeup

Send your submission (writeup and result.lua) to your corresponding TA by the deadline.
Merge your solutions to section 1 with the writeup from section 2. Include a link to the
trained model file in the email. Please use the following title for your email.
[DS-GA-1008 YOUR_TEAM_NAME] Submission A1

* Week 3 studying
** TODO Go over NN notebook 
https://nbviewer.jupyter.org/github/mayanks43/NYU-DL-2016/blob/master/NN.ipynb
** TODO Go over the CNNs notebook
https://nbviewer.jupyter.org/github/mayanks43/NYU-DL-2016/blob/master/CNNs.ipynb



* DONE Homework 2
https://d1b10bmlvqabco.cloudfront.net/attach/iicl1y199833v4/ie3efsaul1h3xo/iksnmh1oi53c/dsga1008a2.pdf
https://docs.google.com/document/d/1U03sfWe_0SqD40P51h19OMK_l3Nx2phmH0Fs0ilX2bs/edit
** DONE Setup AWS

account number 398454964695

https://398454964695.signin.aws.amazon.com/console

pip instal awscli
failed
failed with sudo too
sudo pip install --upgrade pip
fail
found https://github.com/pypa/pip/issues/3165
sudo pip install awscli --upgrade --ignore-installed six

*** usage

aws ec2 start-instances --instance-ids i-f90f127c --region us-east-1

ssh -i ~/Desktop/leconv/leconv.pem ubuntu@ec2-54-84-219-83.compute-1.amazonaws.com

aws ec2 stop-instances --instance-ids i-f90f127c --region us-east-1

** DONE install CUDA
http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-mac-os-x
export PATH=/Developer/NVIDIA/CUDA-7.0/bin:$PATH
export DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-7.0/lib:$DYLD_LIBRARY_PATH

xcode-select --install
luarocks install cutorch
luarocks install cunn

cudnn doesn't work. I wonder if it's installed on aws?

** Math problem 1
** Math problem 2
paper
http://arxiv.org/pdf/1502.03167v3.pdf
*** part 1
just calculus
*** part 2
I think this is in the paper
** papers to read
**** Cropping patches, doing clustering / auto-encoding as pre-training.
http://arxiv.org/abs/1412.6597 
**** Surrogate classes: exemplar CNN
Discriminative Unsupervised Feature Learning with Convolutional Neural Networks
http://papers.nips.cc/paper/5548-discriminative-unsupervised-feature-learning-with-convolutional-neural-networks.pdf 
**** Convolutional K-means: 
http://arxiv.org/pdf/1511.06241v2.pdf 
**** Universum Prescription
http://arxiv.org/pdf/1511.03719.pdf 
**** STACKED WHAT-WHERE AUTO-ENCODERS
http://arxiv.org/pdf/1506.02351.pdf
**** AN ANALYSIS OF UNSUPERVISED PRE-TRAINING IN LIGHT OF RECENT ADVANCES
http://arxiv.org/pdf/1412.6597v4.pdf
** DONE prepare experimental environment
** DONE do full evaluation without unlabeled data
** DONE parse unlabeled data
** DONE do full evaluation without unlabeled data
** DONE parse unlabeled data
** DONE Write result.lua
** DONE refactor provider.lua to load data separately.
There isn't enough memory to load all the data into memory on AWS.
** TODO Pseudo labels
*** DONE figure out the annealing schedule
what it t? It's the epoch.

with Î± = 3, T1 = 100, T2 = 600 without pre-training,
T1 = 200, T2 = 800 with DAE.

if we do 300 epochs...i dunno
maybe make it proportional?


*** DONE What model did they use with pseudo labels in their MNIST paper?
they just use one relu hidden unit + sigmoid for output

we're not going to use this...
*** TODO How much unlabeled data should we use?
They used 100,600,1000,3000 training, 1000 for validation and "the rest" for unlabeled.

There are 60000 labeled images in mnist.
did they really used 3000 labeled for training, and 56000 unlabeled?
Yeesh.

I guess I'll just have to experiment with this.

*** DONE do we have exponentially decaying learning rate already?
yes
*** DONE does our learning rate use momentum?
yes
*** DONE we use dropout?
yes, on both models
*** TODO write code to sample unlabled data
*** DONE should unlabeled data be normalized?

*** TODO Create the unlabeled mask
*** TODO write up the criterion
include annealing schedule and debug statements

not sure how the labeled and unlabeled data are combined in the loss function...

The problem is taht you don't specify the loss function, you specify the gradient
of the loss function with respect to the inputs.

but since the paper says you just multiply the loss by a constant, I think you
just multiply each gradient term by a constant.

it may be as simple as making an if-statement based on whether or not the input has a label.
if it doesn't, make its 

computing the output and gradient normally on the labeled data,
and doing the unlabeled data seperately. Then, you 


make sure the unlabeled data are mixed in with the training data
give them nil labels, too
create a mask vector that indicates where the fake data is

in the training function
calculate the annealing constant

in feval

- update the targets tensor with the outputs of the unlabeled data with a the mask.
- update the gradient by multiplying the mask and the annealing constant
df_do = anneal*mask*df_do

how do you multiply tensors?

anneal_mask = torch.mul(mask, anneal)
M:mm(, y)

*** TODO use cross entropy?
*** TODO experiment with equal unlabeled data (4000)
*** TODO try 16000 unlabeled
*** TODO try 64000 unlabeled
*** TODO try all 100000 unlabeld

** TODO write tsne script
** improve data augmentation

** Try to get cudnn to work
** Unlabeled data experiments
** Visualization
*** Visualizing filters and augmentations
**** Visualize filters in first layer
**** A sample of the 'augmentations' organized in a grid
huh??
*** t-SNE
**** Read t-SNE tutorial
**** cluster testing images using features from your model
** Create writeup
** Submit kaggle code
** bundle code and writeup for submission

* TODO Homework 4
** hpc
ssh akp258@hpc.nyu.edu
ssh mercer

** first_training
lstm no params tweaked

ran for 5 epochs

Took 227 minutes

qsub qjob.sh 
8175105

qstat -u akp258

can't tell if it's runnig correctly.
doesn't seem to be logging to debug.log....

it's writing to pinesol_rnn.8175105

check on this job in the morning
it worked!

DONE lowercase the input
DONE 'word not in vocab' is still blank'
DONE why so many <unk>s?
limited vocab map (10k words), training text has lots more

step: 11605
epoch: 4.9956952216961
epoch = 4.998, train perp. = 79.598, wps = 341, dw:norm() = 5.443, lr = 1.000, since beginning = 227 mins.
==> saving model to /home/akp258/deeplearning/hw4/results/exp_0423234811/model.net
step: 11610
epoch: 4.997847610848
Validation set perplexity : 128.882

output

Query: len word1 word2 etc	
10 how are you I am 
how are you i am truly <eos> it will be available if you 'd have 	
Query: len word1 word2 etc	
5 if we had only known i would have
if we had only known i would have a lot of six-month products 	
Query: len word1 word2 etc	
20 if we had only known
if we had only known of more than $ N it stands for $ N after the government can go to work <eos> at N 	
Query: len word1 word2 etc	
20 if we had only known
if we had only known the transportation department associated with the imf 's workers trying to obtain hbo <eos> one news problem is very positive 	
Query: len word1 word2 etc	
10 my sister is
my sister is n't an equity fund had a cash surplus <eos> mr. 	
Query: len word1 word2 etc	
20 my sister is
my sister is companies under place <eos> they say are the poorest bids told trouble for N the <unk> and the growing number 	
Query: len word1 word2 etc	
10 if you live in new York
if you live in new york city donations so <unk> are an enormous opportunity <eos> later 	
10 if you live in new York
if you live in new york and it has a bigger savings industry not because we 

saved model as "first_training"


*** test run

Saving results at /home/akp258/deeplearning/hw4/results/exp_0425213247	
Writing options to /home/akp258/deeplearning/hw4/results/exp_0425213247/options.log	
TEST MODE	
Loading model file from results/first_training/model.net	
Testing model...	
 [================================================= 82429/82429 ============================================>]ETA: 0ms | Step: 10ms          
Test set perplexity : 126.090

better than 150 baseline!

** gru training

training for 5 hours as a test to ensure I did it right. 
Should be about as good as lstm (e.g. 120 perp)

th result.lua --mode=train --max_epoch=5 --model_type=gru

qsub qjob.sh 
8186299

exp_0425221118

qstat -u akp258

changed the results directory to be 'initial_gru'

epoch = 4.994, train perp. = 262.332, wps = 390, dw:norm() = 105.236, lr = 1.000, since beginning = 198 mins.       
==> saving model to /home/akp258/deeplearning/hw4/results/exp_0425221118/model.net  
step: 11600 
epoch: 4.9935428325441      
step: 11610 
epoch: 4.997847610848       
Validating model... 
Validation set perplexity : 335.156 
==> saving model to /home/akp258/deeplearning/hw4/results/exp_0425221118/model.net

validation perp 335

over twice as bad as lstm...

looks like it overfit.

best validation perp during training:

epoch: 1.9974171330176
Validating model...
Validation set perplexity : 191.696


** LSTM 10 epochs

no default param changes, except 10 epochs instead of 5

lstm model.

th result.lua --mode=train --max_epoch=10 --model_type=lstm --exp_name=lstm10

8186459

*** check how lstm10 did


epoch = 9.987, train perp. = 41.263, wps = 600, dw:norm() = 7.497, lr = 0.031, since beginning = 257 mins.  
==> saving model to /home/akp258/deeplearning/hw4/results/lstm10_0425223629/model.net       
step: 23200 
epoch: 9.9870856650882      
step: 23210 
epoch: 9.9913904433922      
step: 23220 
epoch: 9.9956952216961      
Validating model... 
Validation set perplexity : 122.700 
==> saving model to /home/akp258/deeplearning/hw4/results/lstm10_0425223629/model.net 

validation perp 122.7

** LSTM 10 epochs, 4 layers

th result.lua --mode=train --max_epoch=10 --model_type=lstm --layers=4 --exp_name=lstm.10e.4l

job ID: 8193394

epoch = 9.987, train perp. = 58.572, wps = 326, dw:norm() = 6.742, lr = 0.031, since beginning = 474 mins.
==> saving model to /home/akp258/deeplearning/hw4/results/lstm.10e.4l_0426190607/model.net
step: 23200
epoch: 9.9870856650882
step: 23210
epoch: 9.9913904433922
step: 23220
epoch: 9.9956952216961
Validating model...
Validation set perplexity : 125.830
==> saving model to /home/akp258/deeplearning/hw4/results/lstm.10e.4l_0426190607/model.net

no intelligent saving, but no evidence of overfitting, either. validation keeps going down.

** gru max epoch 10, dropout 0.5

th result.lua --mode=train --max_epoch=10 --model_type=gru --dropout=0.5 --exp_name=gru.10e.drop

job number 8194547

file: results/gru.10e.drop_0426222640/debug.log

best perp after 1 epoch, then did worse afterwards:

epoch = 0.990, train perp. = 516.818, wps = 481, dw:norm() = 8.598, lr = 1.000, since beginning = 32 mins.
Validating model...
Validation set perplexity : 257.568

final perplexity after 10 epochs:

epoch = 9.987, train perp. = 436.595, wps = 470, dw:norm() = 509.383, lr = 0.031, since beginning = 329 mins.
Validating model...
Validation set perplexity : 337.339


** lstm max epoch 10, dropout 0.5


th result.lua --mode=train --max_epoch=10 --model_type=lstm --dropout=0.5 --exp_name=lstm.10e.drop

job number 8194557


epoch = 9.987, train perp. = 127.609, wps = 410, dw:norm() = 5.621, lr = 0.031, since beginning = 376 mins.
Validating model...
Validation set perplexity : 122.492
==> New perplexity 886.36252293709 < previous value: 887.51813195027
==> saving model to /home/akp258/deeplearning/hw4/results/lstm.10e.drop_0426222835/model.net

not obviously better

** gru max epoch 10, learning rate of 0.5

th result.lua --mode=train --max_epoch=10 --model_type=gru --lr=0.5 --exp_name=gru.10e.lr5

job number 8194559

results/gru.10e.lr5_0426222930/model.net

no overfitting, the validation goes down consitently until the end.

epoch = 9.987, train perp. = 38.020, wps = 439, dw:norm() = 16.317, lr = 0.016, since beginning = 352 mins.
Validating model...
Validation set perplexity : 131.183

close to LSTM.

** lstm max epoch 10, learning rate of 0.5

th result.lua --mode=train --max_epoch=10 --model_type=lstm --lr=0.5 --exp_name=lstm.10e.lr5

job number 8194562

epoch = 9.987, train perp. = 51.454, wps = 368, dw:norm() = 9.594, lr = 0.016, since beginning = 419 mins.
Validating model...
Validation set perplexity : 118.532
==> New perplexity 880.30464523851 < previous value: 881.14051476258
==> saving model to /home/akp258/deeplearning/hw4/results/lstm.10e.lr5_0426223232/model.net

similarly better

** TODO gru max epoch 10, learning rate of 0.2

learning rate helped, gonna see if this helps more.

th result.lua --mode=train --max_epoch=10 --model_type=gru --lr=0.2 --exp_name=gru.10e.lr2

job number 8200785

** TODO lstm max epoch 10 rnn size 300

drop out and learning rate don't do very much.
trying rrn size.

th result.lua --mode=train --max_epoch=10 --model_type=lstm --rnn_size=300 --exp_name=lstm.10e.rnn300

Job number 8200809

** TODO lstm max epoch 15 rnn size 300

I'm assuming a bigger rnn size will be better, but take longer to train. adding 5 epochs to test.

th result.lua --mode=train --max_epoch=15 --model_type=lstm --rnn_size=300 --exp_name=lstm.15e.rnn300

job number 8200849

** TODO gru max epoch 10 learning rate 0.5 rnn size 300

th result.lua --mode=train --max_epoch=10 --model_type=gru --lr=0.5 --rnn_size=300 --exp_name=gru.10e.lr5.rnn300

job number 8200821


** TODO lstm max epoch 10 grad clipping 3

th result.lua --mode=train --max_epoch=10 --model_type=lstm --max_grad_norm=3 --exp_name=lstm.10e.grad3

job number 8202242

** TODO gru max epoch 10 grad clipping 3

th result.lua --mode=train --max_epoch=10 --model_type=gru --max_grad_norm=3 --exp_name=gru.10e.grad3

job number 8202266

** DONE are nngraph_warmup.lua, result.lua and query_sentences.lua ready to be submitted?
** DONE add picture to your results
** TODO do run_test on the models with the best validation perplexity to check you made the benchmark.
** TODO write up
*** TODO how do you load the style file?
*** DONE problem 1
*** TODO Add in the results of the remaining experiments
*** TODO write up the test results of the best model.
** TODO check that result.lua matches your writeup results.
** TODO run query sentences on the best model as a sanity check.
** TODO move best model to public directory
** TODO add best model to result.lua
** TODO add best model to query_sentences.lua
** TODO move the 3 files to a zip folder.
** TODO create a readme
** TODO email the TA




