{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Homework 1\n",
    "Maya Rotmensch (mer567), Alex Pine (akp258)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1: Warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are given the following definition of $ x_{out} $:\n",
    "\n",
    "$$ x_{out} = \\sigma(x_{in}) = \\frac{1}{1 + e^{-x_{in}}} $$\n",
    "\n",
    "Taking it's derivative with respect to $ x_{in} $ we get:\n",
    "\n",
    "$$ \\frac{\\partial x_{out}}{\\partial x_{in}} = -(1+e^{-x_{in}})^{-2} (-e^{-x_{in}}) $$ \n",
    "\n",
    "$$ \\frac{\\partial x_{out}}{\\partial x_{in}} = \\frac{e^{-x_{in}}}{(1 + e^{-x_{in}})^2} $$\n",
    "\n",
    "$E$ is a function of $x_{out}$ by definition, and $x_{out} = \\sigma(x_{in})$. Therefore we can apply the Chain rule to  find $ \\frac{\\partial E}{\\partial x_{in}} $:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial x_{in}} = \\frac{\\partial E}{\\partial x_{out}} \\frac{\\partial x_{out}}{\\partial x_{in}} $$\n",
    "\n",
    "Since we've found $\\frac{\\partial x_{out}}{\\partial x_{in}}$, and the problem states that $\\frac{\\partial E}{\\partial x_{out}}$ is given, we get \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial x_{in}} = \\frac{\\partial E}{\\partial x_{out}} \\frac{e^{-x_{in}}}{(1 + e^{-x_{in}})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the following definition for the i-th component of a vector $ X_{out} $:\n",
    "\n",
    "$$ (X_{out})_i = \\frac{e^{-\\beta(X_{in})_i}}{\\sum_k e^{-\\beta(X_{in})_k}} $$\n",
    "\n",
    "We are asked to find $ \\frac{\\partial (X_{out})_i}{\\partial (X_{in})_j} $.\n",
    "\n",
    "Let's define some new variables for notational simplicity:\n",
    "\n",
    "$$ X_i = (X_{in})_i $$\n",
    "\n",
    "$$ f(X_i) = e^{-\\beta X_i}, g(X) = \\sum_k f(X_k) $$\n",
    "\n",
    "$$ Y_i = (X_{out})_i = \\frac{f(X_i)}{g(X)} $$\n",
    "\n",
    "$$ J_{ij} = \\frac{\\partial (X_{out})_i}{\\partial (X_{in})_j} = \\frac{\\partial Y_i}{\\partial X_j} $$\n",
    "\n",
    "In this context, the Quotient Rule is:\n",
    "\n",
    "$$ J_{ij} = \\frac{ g(X) \\frac{\\partial f(X_i)}{\\partial X_j} - f(X_i) \\frac{\\partial g(X)}{\\partial X_j} }{g(X)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $ i = j $, then \n",
    "\n",
    "$$ J_{ij} = J_{ii} = \\frac{ g(X) \\frac{\\partial f(X_i)}{\\partial X_i} - f(X_i) \\frac{\\partial g(X)}{\\partial X_i} }{g(X)^2} $$\n",
    "\n",
    "First we solve for $\\frac{\\partial f(X_i)}{\\partial X_i}$ and $\\frac{\\partial g(X)}{\\partial X_i}$:\n",
    "\n",
    "$$ \\frac{\\partial f(X_i)}{\\partial X_i} = -\\beta e^{-\\beta X_i} = -\\beta f(X_i) $$\n",
    "\n",
    "$$ \\frac{\\partial g(X)}{\\partial X_i} = -\\beta f(X_i) $$\n",
    "\n",
    "Substituting these back into $J_{ij}$, we get:\n",
    "\n",
    "$$ J_{ij} = \\frac{ g(X) (-\\beta f(X_i)) - f(X_i) (-\\beta f(X_i)) }{g(X)^2} $$\n",
    "\n",
    "$$ J_{ij} = -\\beta \\frac{f(X_i)}{g(X)} + \\beta \\frac{f(X_i)^2}{g(X)^2} $$\n",
    "\n",
    "Recognizing that $Y_i = \\frac{f(X_i)}{g(X)}$, this becomes:\n",
    "\n",
    "$$ J_{ij} = -\\beta Y_i(1-Y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $ i \\neq j $, then \n",
    "\n",
    "$$ J_{ij} = \\frac{ g(X) \\frac{\\partial f(X_i)}{\\partial X_j} - f(X_i) \\frac{\\partial g(X)}{\\partial X_j} }{g(X)^2} $$\n",
    "\n",
    "Now that the derivatives are in regard to $X_j$ instead of $X_j$, we get:\n",
    "\n",
    "$$ \\frac{\\partial f(X_i)}{\\partial X_j} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial g(X)}{\\partial X_j} = -\\beta e^{-\\beta X_j} = -\\beta f(X_j) $$\n",
    "\n",
    "Substituting these back into $J_{ij}$, we get:\n",
    "\n",
    "$$ J_{ij} = \\frac{ - f(X_i) (-\\beta f(X_j)) }{g(X)^2} $$ \n",
    "\n",
    "$$ J_{ij} = \\beta \\frac{f(X_i)}{g(X)} \\frac{f(X_j)}{g(X)} $$\n",
    "\n",
    "$$ J_{ij} = \\beta Y_i Y_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the original notation of the problem, the solution can be summarized as:\n",
    "\n",
    "$$ \\frac{\\partial (X_{out})_i}{\\partial (X_{in})_j} = -\\beta (X_{out})_i(1-(X_{out})_i), \\text{ when } i = j $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial (X_{out})_i}{\\partial (X_{in})_j} = \\beta (X_{out})_i(X_{out})_j, \\text{ when } i \\neq j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
